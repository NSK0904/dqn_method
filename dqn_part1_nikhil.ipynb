{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a410b0",
   "metadata": {},
   "source": [
    "The code is an implementation of a deep Q-learning algorithm to train an agent to solve the CartPole-v1 environment from the OpenAI Gym using PyTorch. \n",
    "\n",
    "The first part of the code defines the hyperparameters, including the learning rate, the number of episodes, and the discount factor. The size of the hidden layer, the size of the experience replay memory, the batch size, and the epsilon-greedy exploration parameters are also defined. \n",
    "\n",
    "The second part of the code defines a neural network class (NeuralNetwork) to serve as the function approximator for the Q-value function. The neural network consists of two linear layers followed by the tanh activation function. The QNet_Agent class serves as an agent that uses the neural network to select actions and optimize the Q-value function. \n",
    "\n",
    "The third part of the code defines an ExperienceReplay class that serves as a buffer to store the agent's experiences. The push method is used to add a new experience to the buffer, and the sample method is used to randomly sample a batch of experiences from the buffer. \n",
    "\n",
    "The code then initializes the experience replay memory, the QNet_Agent, and the environment. It then enters a loop to run the training process for a specified number of episodes. \n",
    "\n",
    "For each episode, the agent selects an action based on the current state of the environment and the current epsilon value (for epsilon-greedy exploration). The agent then takes a step in the environment, adds the experience to the replay memory, and optimizes the Q-value function using a batch of experiences randomly sampled from the replay memory. \n",
    "\n",
    "The code also tracks the total number of steps taken and the total number of frames rendered during the training process. The training process terminates if the agent successfully solves the environment (i.e., reaches an average reward of 195 over the last 100 episodes) or if the maximum number of episodes is reached. \n",
    "\n",
    "Finally, the code outputs the total time taken for the training process and a plot of the number of steps taken per episode. It also uses the QNet_Agent to play one episode in the environment to visualize the performance of the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78bbf242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after steps: 9\n",
      "\n",
      "*** Episode 0 ***                       \n",
      "Av.reward: [last 10]: 3.50, [last 100]: 0.35, [all]: 35.00                       \n",
      "epsilon: 0.84, frames_total: 35\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: 21.70, [last 100]: 2.52, [all]: 22.91                       \n",
      "epsilon: 0.54, frames_total: 252\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: 170.10, [last 100]: 19.53, [all]: 93.00                       \n",
      "epsilon: 0.02, frames_total: 1953\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: 238.70, [last 100]: 43.40, [all]: 140.00                       \n",
      "epsilon: 0.00, frames_total: 4340\n",
      "Elapsed time:  00:00:04\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: 226.90, [last 100]: 66.09, [all]: 161.20                       \n",
      "epsilon: 0.00, frames_total: 6609\n",
      "Elapsed time:  00:00:07\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: 454.10, [last 100]: 111.50, [all]: 218.63                       \n",
      "epsilon: 0.00, frames_total: 11150\n",
      "Elapsed time:  00:00:11\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: 551.00, [last 100]: 166.60, [all]: 273.11                       \n",
      "epsilon: 0.00, frames_total: 16660\n",
      "Elapsed time:  00:00:17\n",
      "SOLVED! After 66 episodes \n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: 524.50, [last 100]: 219.05, [all]: 308.52                       \n",
      "epsilon: 0.00, frames_total: 21905\n",
      "Elapsed time:  00:00:23\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: 894.60, [last 100]: 308.51, [all]: 380.88                       \n",
      "epsilon: 0.00, frames_total: 30851\n",
      "Elapsed time:  00:00:35\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: 359.30, [last 100]: 344.44, [all]: 378.51                       \n",
      "epsilon: 0.00, frames_total: 34444\n",
      "Elapsed time:  00:00:40\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average reward: 383.64\n",
      "Average reward (last 100 episodes): 383.64\n",
      "Solved after 66 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after steps: 261\n",
      "Finished after steps: 268\n",
      "Finished after steps: 268\n",
      "Finished after steps: 259\n",
      "Finished after steps: 251\n",
      "Finished after steps: 263\n",
      "Finished after steps: 262\n",
      "Finished after steps: 272\n",
      "Finished after steps: 252\n",
      "Finished after steps: 263\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch                          # Importing the PyTorch library\n",
    "import torch.nn as nn                 # Importing the neural network module of PyTorch\n",
    "import torch.optim as optim           # Importing the optimization module of PyTorch\n",
    "import gym                            # Importing the OpenAI gym library\n",
    "import random                         # Importing the random module\n",
    "import math                           # Importing the math module\n",
    "import time                           # Importing the time module\n",
    "\n",
    "import matplotlib.pyplot as plt       # Importing the pyplot module from the matplotlib library\n",
    "\n",
    "use_cuda = torch.cuda.is_available()  # Checking if CUDA is available and setting a flag accordingly\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")  # Assigning the device to be used for computations based on the availability of CUDA\n",
    "\n",
    "Tensor = torch.Tensor                # Creating a shortcut for the Tensor class of PyTorch\n",
    "LongTensor = torch.LongTensor        # Creating a shortcut for the LongTensor class of PyTorch\n",
    "\n",
    "env = gym.make('CartPole-v1')         # Creating an instance of the CartPole-v1 environment from OpenAI gym\n",
    "\n",
    "seed_value = 23                       # Setting a seed value to reproduce the same random results\n",
    "torch.manual_seed(seed_value)         # Setting the seed for the PyTorch library\n",
    "random.seed(seed_value)               # Setting the seed for the random module\n",
    "\n",
    "\n",
    "learning_rate = 0.02                  # Setting the learning rate for the optimizer\n",
    "num_episodes = 100                    # Setting the number of episodes to run the agent for\n",
    "gamma = 1                             # Setting the discount factor for the reward\n",
    "\n",
    "hidden_layer = 64                     # Setting the size of the hidden layer of the neural network\n",
    "\n",
    "replay_mem_size = 50000               # Setting the size of the replay memory\n",
    "batch_size = 32                       # Setting the batch size for the optimizer\n",
    "\n",
    "egreedy = 0.9                         # Setting the initial value of the epsilon-greedy parameter\n",
    "egreedy_final = 0                     # Setting the final value of the epsilon-greedy parameter\n",
    "egreedy_decay = 500                   # Setting the decay factor for the epsilon-greedy parameter\n",
    "\n",
    "report_interval = 10                  # Setting the number of episodes after which to report the score\n",
    "score_to_solve = 195                  # Setting the minimum score required to solve the environment\n",
    "\n",
    "# Get the number of inputs and outputs for the environment\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n \n",
    "\n",
    "# Define a function to calculate the epsilon value for the e-greedy policy\n",
    "def calculate_epsilon(steps_done):\n",
    "    # Calculate epsilon value based on the current number of steps taken\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon\n",
    "\n",
    "# Define a neural network class that extends the PyTorch nn.Module class\n",
    "class NeuralNetwork(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Define the first linear layer with the specified number of inputs and hidden layer size\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer) \n",
    "        # Define the second linear layer with the hidden layer size and the number of outputs\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_outputs) \n",
    "        # Define the activation function to use\n",
    "        self.activation = nn.Tanh()\n",
    "             \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the first linear layer and the activation function\n",
    "        output1 = self.linear1(x) \n",
    "        output1 = self.activation(output1)\n",
    "        # Pass the output of the first layer through the second linear layer to get the final output\n",
    "        output2 = self.linear2(output1)\n",
    "        return output2\n",
    "\n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        # Instantiate the neural network and move it to the device (CPU or GPU)\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        # Define the loss function to be mean squared error\n",
    "        self.loss_func = nn.MSELoss()      \n",
    "        # Define the optimizer to be Adam, using the learning rate defined earlier\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        # Generate a random number between 0 and 1 to decide whether to choose a random action (for exploration)\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon: # Choose the best action predicted by the neural network\n",
    "            \n",
    "            with torch.no_grad(): # Turn off gradients since we are not training the network in this step\n",
    "                \n",
    "                state = Tensor(state).to(device) # Convert the state to a tensor and move it to the device\n",
    "                action_from_nn = self.nn(state) # Pass the state to the neural network to get the Q values for each action\n",
    "                action = torch.max(action_from_nn,0)[1] # Choose the action with the highest Q value\n",
    "                action = action.item() # Convert the action tensor to a scalar\n",
    "        else: # Choose a random action\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # If there are not enough memories in the replay memory, skip this step\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        # Sample a batch of memories from the replay memory\n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        # Convert the data to tensors and move them to the device\n",
    "        state = Tensor(state).to(device) \n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "        # Compute the Q values for the new states, but do not compute gradients for this step\n",
    "        new_state_values = self.nn(new_state).detach()\n",
    "        # Choose the maximum Q value for each new state\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        # Compute the target value for each state\n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        # Compute the Q value for each state-action pair\n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute the loss between the predicted Q values and the target Q values\n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        # Zero out the gradients in the optimizer\n",
    "        self.optimizer.zero_grad()\n",
    "        # Backpropagate the loss to compute gradients\n",
    "        loss.backward()\n",
    "        # Update the weights of the neural network using the gradients\n",
    "        self.optimizer.step()\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # capacity of the replay memory\n",
    "        self.memory = []          # initialize the replay memory\n",
    "        self.position = 0         # starting position of the memory\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        # Add a new transition to the replay memory\n",
    "        \n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        # If the replay memory is not full, append the new transition to the end\n",
    "        if self.position >= len(self.memory): \n",
    "            self.memory.append(transition)\n",
    "        # If the replay memory is full, overwrite the oldest transition with the new one\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        self.position = ( self.position + 1 ) % self.capacity  # update the current position in the memory\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        # Sample a batch of transitions from the replay memory\n",
    "        \n",
    "        # Randomly select batch_size transitions from the replay memory\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Transpose the batch of transitions so that the states, actions, etc. are grouped together\n",
    "        # (i.e. separate lists for states, actions, etc.)\n",
    "        return zip(*samples)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the length of the replay memory\n",
    "        \n",
    "        return len(self.memory)\n",
    "\n",
    "# Create a memory object with the specified replay memory size\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "\n",
    "# Create a QNet_Agent object\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "# Put the neural network in evaluation mode, which disables certain operations such as dropout\n",
    "qnet_agent.nn.eval()\n",
    "\n",
    "# Create an environment object with the 'CartPole-v1' environment and enable rendering\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Repeat the following for the specified number of episodes\n",
    "for episode in range(10):\n",
    "\n",
    "    # Reset the environment and get the initial state and associated variables\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Pause for a moment to allow the user to view the initial state\n",
    "    time.sleep(1.)\n",
    "\n",
    "    # Repeat the following for a maximum of 10000 steps\n",
    "    for step in range(10000):\n",
    "\n",
    "        # Render the current state of the environment to the screen\n",
    "        env.render()\n",
    "\n",
    "        # Pause for a moment to allow the user to view the current state\n",
    "        time.sleep(0.02)\n",
    "\n",
    "        # Choose an action based on the current state using the QNet_Agent object with epsilon set to 0, meaning no exploration\n",
    "        action = qnet_agent.select_action(state, 0)\n",
    "\n",
    "        # Take the chosen action and get the resulting state, reward, and whether the episode is finished\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Update the current state to the new state\n",
    "        state = new_state\n",
    "\n",
    "        # If the episode is finished, print the number of steps taken and break out of the loop\n",
    "        if done:\n",
    "            print('Finished after steps:', step)\n",
    "            break\n",
    "\n",
    "    # Break out of the loop after the first episode\n",
    "    break\n",
    "\n",
    "# Close the environment and the underlying OpenGL context\n",
    "env.close()\n",
    "env.env.close()\n",
    "\n",
    "#Create a gym environment for CartPole-v1 game\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#Set the neural network to training mode\n",
    "qnet_agent.nn.train()\n",
    "\n",
    "#Create empty list to store the total number of steps for each episode\n",
    "steps_total = []\n",
    "\n",
    "#Initialize the total number of frames seen to 0\n",
    "frames_total = 0\n",
    "\n",
    "#Set a variable to track how many episodes were completed before the game is solved\n",
    "solved_after = 0\n",
    "\n",
    "#Set a boolean variable to track if the game is solved\n",
    "solved = False\n",
    "\n",
    "#Record the starting time of the game\n",
    "start_time = time.time()\n",
    "\n",
    "# loop through the specified number of episodes\n",
    "for i_episode in range(num_episodes):\n",
    "\n",
    "    # reset the environment and get the initial state\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    # set the initial step count to 0\n",
    "    step = 0 \n",
    "\n",
    "    # loop until the episode is done\n",
    "    while True: \n",
    "        \n",
    "        # increment the step count and total frame count\n",
    "        step += 1 \n",
    "        frames_total += 1 \n",
    "        \n",
    "        # calculate the current value of epsilon for the exploration-exploitation tradeoff\n",
    "        epsilon = calculate_epsilon(frames_total) \n",
    "        \n",
    "        # select an action using the Q-network agent's select_action() method, passing in the current state and epsilon value\n",
    "        action = qnet_agent.select_action(state, epsilon) \n",
    "        \n",
    "        # take a step in the environment with the selected action and get the resulting new state, reward, and done flag\n",
    "        new_state, reward, done, _, _ = env.step(action) \n",
    "\n",
    "        # add the current state, action, new state, reward, and done flag to the experience replay memory\n",
    "        memory.push(state, action, new_state, reward, done) \n",
    "        \n",
    "        # optimize the Q-network agent's network weights using the experience replay memory and backpropagation\n",
    "        qnet_agent.optimize() \n",
    "        \n",
    "        # set the current state to the new state for the next iteration of the loop\n",
    "        state = new_state \n",
    "         \n",
    "        # if the episode is done\n",
    "        if done: \n",
    "            # append the final step count to the list of step counts for this run\n",
    "            steps_total.append(step) \n",
    "            \n",
    "            # calculate the mean reward over the last 100 steps\n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            # if the mean reward over the last 100 steps is greater than the threshold for solving the environment and the environment has not yet been solved\n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                # print a message indicating that the environment has been solved and at what episode\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            # if the current episode number is a multiple of the report interval\n",
    "            if (i_episode % report_interval == 0):\n",
    "                \n",
    "                # print a message indicating the current episode number, the average reward over the last n steps, \n",
    "                #the mean reward over the last 100 steps, the overall mean reward, epsilon, and total frame count\n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(steps_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(steps_total)/len(steps_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                # calculate and print the elapsed time since the start of training\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "            # break out of the while loop since the episode is finished\n",
    "            break\n",
    "\n",
    "state_dict = qnet_agent.nn.state_dict()  # Get the state dictionary of the neural network of the agent\n",
    "torch.save(state_dict, 'dqn_er4.pth')  # Save the state dictionary to a file named 'dqn_er4.pth'\n",
    "\n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))  # Print the average reward across all episodes\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))  # Print the average reward across the last 100 episodes\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)  # If the problem is solved, print the episode number where it was solved\n",
    "plt.figure(figsize=(12,5))  # Create a new plot with the given size\n",
    "plt.title(\"Rewards\")  # Set the title of the plot to \"Rewards\"\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green', width=5)  # Plot the rewards over time as a bar chart\n",
    "plt.show()  # Show the plot\n",
    "\n",
    "env.close()  # Close the OpenAI Gym environment\n",
    "env.env.close()  # Close the underlying environment (not strictly necessary, but it's a good practice to avoid potential issues)\n",
    "\n",
    "# Load the saved state dictionary of the trained neural network\n",
    "state_dict = torch.load('dqn_er4.pth')\n",
    "# Load the state dictionary into the QNet_Agent's neural network\n",
    "qnet_agent.nn.load_state_dict(state_dict)\n",
    "# Set the QNet_Agent's neural network to evaluation mode\n",
    "qnet_agent.nn.eval()\n",
    "# Create a gym environment for the CartPole problem with human rendering mode\n",
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "\n",
    "# Run 10 episodes\n",
    "for episode in range(10):\n",
    "    # Reset the environment to obtain initial state\n",
    "    state, _ = env.reset()\n",
    "    # Wait for 1 second before the episode begins\n",
    "    time.sleep(1.) \n",
    "    # Run up to 10000 steps in the episode\n",
    "    for step in range(10000):\n",
    "        # Render the environment\n",
    "        env.render() \n",
    "        # Wait for 0.02 seconds before taking an action\n",
    "        time.sleep(0.02) \n",
    "        # Select an action using the QNet_Agent with epsilon = 0 (greedy policy)\n",
    "        action = qnet_agent.select_action(state, 0)\n",
    "        # Take the selected action and obtain the next state, reward, done flag, and information\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        # Update the current state to the new state\n",
    "        state = new_state\n",
    "        # If the episode is done (pole has fallen or reached 200 steps), print the number of steps and break\n",
    "        if done:\n",
    "            print('Finished after steps:', step)\n",
    "            break\n",
    "# Close the environment and the environment's underlying core\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c14c4b",
   "metadata": {},
   "source": [
    "# Explanation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee7865",
   "metadata": {},
   "source": [
    "The results show that the agent was able to solve the environment after 66 episodes. \n",
    "\n",
    "The average reward increased significantly over the course of the training, from 35 in the first 10 episodes before settling around 383.64 after the 100th episode. This suggests that the agent gradually learned how to perform better in the environment, improving its performance over time.\n",
    "\n",
    "The epsilon value, which controls the exploration vs. exploitation trade-off, decreases over time. In the first episode, the epsilon value is 0.84, indicating that the agent is exploring the environment more than exploiting its knowledge. However, by the 20th episode, the epsilon value has decreased to 0.02, indicating that the agent is exploiting its knowledge of the environment more than exploring. This is likely due to the fact that the agent is learning more about the environment and can make more informed decisions.\n",
    "\n",
    "The frames_total value, which indicates the total number of frames the agent has processed, increases over time. This is expected since the agent is continuously interacting with the environment and learning from it.\n",
    "\n",
    "Finally, there are some variations in the number of steps required to finish the training, as indicated by the \"Finished after steps\" messages at the end of the output. This is because the training process involves some randomness, and the exact number of steps required to solve the environment can vary between different runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b3640",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6375f",
   "metadata": {},
   "source": [
    "There are 9 hyperparameters that can be tuned for best results.\n",
    "\n",
    "'learning_rate': Learning rate is a hyperparameter that controls how much the weights of the neural network are adjusted with each update. It is a crucial parameter in training a neural network, as a value that is too high can cause the weights to oscillate and diverge, while a value that is too low can lead to slow convergence. In this code, the learning rate is set to 0.02. To test for best results I will use 0.01 and 0.001.\n",
    "\n",
    "'num_episodes': The number of episodes is the total number of times the agent plays the game. One episode consists of the agent starting at a random state and taking actions until it reaches a terminal state (i.e., the game is won or lost). The agent's performance is evaluated at the end of each episode, and the weights of the neural network are updated based on the experience gained during that episode. The num_episodes hyperparameter controls how many times the agent will play the game and, therefore, the number of times the weights of the neural network will be updated. In this code, the number of episodes is set to 100. To test for best results I will use 150 and 200.\n",
    "\n",
    "'gamma': Gamma is a hyperparameter that controls the discount factor for future rewards in the reinforcement learning algorithm. It is a value between 0 and 1 and determines how much weight the agent places on future rewards. A value of 0 means that the agent only considers immediate rewards, while a value of 1 means that the agent considers all future rewards equally. In this code, gamma is set to 1. To test for best results I will use 0.99.\n",
    "\n",
    "'hidden_layer': The hidden layer is a layer in the neural network that is not directly connected to the input or output layers. It contains neurons that use activation functions to transform the input data and provide non-linear mappings between the input and output layers. The hidden_layer hyperparameter controls the number of neurons in the hidden layer of the neural network used in this code. In this code, the number of neurons in the hidden layer is set to 64. To test for best results, I will use 128 and 256.\n",
    "\n",
    "'replay_mem_size': Replay memory is a buffer that stores the agent's experiences (i.e., state, action, reward, next state) during gameplay. The agent randomly samples from the replay memory to learn from past experiences and reduce the correlation between consecutive updates. The replay_mem_size hyperparameter controls the size of the buffer used to store the agent's experiences. In this code, the size of the replay memory buffer is set to 50,000. To test for best results, I will use 100,000 and 200,000.\n",
    "\n",
    "'batch_size': This parameter determines the number of transitions sampled from the replay memory to update the Q-network at each training iteration. A larger batch size means more training data will be processed at once, which can help to reduce the variance of the updates and provide more stable training. However, a larger batch size also requires more memory and computational resources. In the given code, the value for batch_size is 32. To test for best results, I will use 64 and 128.\n",
    "\n",
    "'egreedy': This parameter determines the probability of selecting a random action (exploration) instead of the action with the highest Q-value (exploitation) during training. The egreedy parameter is used to balance exploration and exploitation during training. A high value of egreedy means that the agent is more likely to explore and try out new actions, while a low value of egreedy means that the agent is more likely to exploit its current knowledge and choose actions that are believed to be optimal. In the given code, the value for egreedy is 0.9. To test for best results, I will use 0.99 and 1.\n",
    "\n",
    "'egreedy_final': This parameter determines the final value of egreedy after annealing. Annealing is a technique used to gradually reduce the exploration rate during training to encourage the agent to rely more on its learned policy. egreedy_final is the minimum exploration rate that the agent will reach at the end of training. In the given code, the value for egreedy_final is 0. To test for best results, I will use 0.05 and 0.01.\n",
    "\n",
    "'egreedy_decay': This parameter determines the rate at which egreedy is annealed during training. A higher egreedy_decay value means that the agent will explore less over time, while a lower value means that the agent will explore more. In the given code, the value for egreedy_decay is 500. To test for best results, I will use 1000 and 2000.\n",
    "\n",
    "I will run 2 different tests for best results using the values mentionned above for each hyperparameter. This is help me with comparasions on how the results change with changing values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
